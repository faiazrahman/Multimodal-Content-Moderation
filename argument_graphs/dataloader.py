import sys
import os
import logging

import pandas as pd
from tqdm import tqdm
from pprint import pprint

import torch
from torch.utils.data import Dataset, DataLoader

from transformers import AutoTokenizer

logging.basicConfig(level=logging.DEBUG) # DEBUG, INFO, WARNING, ERROR, CRITICAL

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATA_PATH = "./data"
AUC_DATAFRAME_FILE = "auc_dataframe.pkl"
AUC_DATAFRAME_PATH = os.path.join(DATA_PATH, "ArgumentativeUnitClassification", AUC_DATAFRAME_FILE)
RTC_DATAFRAME_FILE = "rtc_dataframe.pkl"
RTC_DATAFRAME_PATH = os.path.join(DATA_PATH, "RelationshipTypeClassification", RTC_DATAFRAME_FILE)

class ArgumentativeUnitClassificationDataset(Dataset):
    """
    torch.utils.data.Dataset for AUC data of the format (text, label), where
    the labels are as follows:
        0   Non-argumentative unit
        1   Claim
        2   Premise

    `__getitem__()` returns a dict containing "text" and "label"
        text:   Encoded input tensor for sequence classification model; encoded
            by the specified tokenizer (which is a `transformers.AutoTokenizer`);
            default is `bert-base-uncased`
        label:  Label tensor (0, 1, or 2)
    """

    def __init__(
        self,
        from_dataframe_pkl_path: str = AUC_DATAFRAME_PATH,
        tokenizer: str = "bert-base-uncased",
    ):
        logging.info("Initializing ArgumentativeUnitClassificationDataset...")
        df = None
        if os.path.exists(from_dataframe_pkl_path):
            df = pd.read_pickle(from_dataframe_pkl_path)
        else:
            raise Exception("AUC dataframe does not exist\nRun argument_graphs/data_preprocessing.py")
        self.data_frame = df

        self.tokenizer = None
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)
        except:
            raise Exception("Invalid model name passed to transformers.AutoTokenizer")

        # Run tokenization for all text during dataset initialization for
        # faster training and to ensure same tensor sizes (via padding and
        # truncation computed over the entire dataset)
        logging.info("Running dataset tokenization for ArgumentativeUnitClassificationDataset...")
        self.encoded_texts = [
            self.tokenizer(
                text,
                padding="max_length", max_length=512, truncation=True,
                return_tensors="pt"
            ) for text in self.data_frame['text']
        ]

    def __len__(self):
        """ Returns the size of the dataset; called as len(dataset) """
        return len(self.data_frame.index)

    def __getitem__(self, idx: int):
        """ Returns a dict containing "text" (encoded tensor) and "label" """
        if torch.is_tensor(idx):
            idx = idx.tolist()

        text = self.data_frame.loc[idx, 'text']
        # encoded_inputs = self.tokenizer(
        #     text, padding=True, truncation=True, return_tensors="pt"
        # )
        encoded_inputs = self.encoded_texts[idx]
        # Note: When passing the direct output of `transformers.AutoTokenizer()`
        # to `transformers.AutoModelForSequenceClassification`, the error
        # `ValueError: too many values to unpack (expected 2)` comes up;
        # This is an issue with there being an additional dimension in the
        # three tensors generated by the tokenizer; the model expects the
        # dimension to be (batch_size, seq_len), but the tokenizer returns
        # (1, batch_size, seq_len); thus, we `.squeeze()` to get rid of that
        # extra dimension
        # https://stackoverflow.com/questions/67193312/huggingface-transformers-returning-valueerror-too-many-values-to-unpack-expec
        encoded_inputs = {
            'input_ids': encoded_inputs['input_ids'].squeeze(),
            'token_type_ids': encoded_inputs['token_type_ids'].squeeze(),
            'attention_mask': encoded_inputs['attention_mask'].squeeze(),
        }
        label = torch.Tensor(
            [self.data_frame.loc[idx, 'label']]
        ).long().squeeze()
        # label = self.data_frame.loc[idx, 'label']

        item = {
            "text": encoded_inputs,
            "label": label,
        }
        return item

class RelationshipTypeClassificationDataset(Dataset):
    """
    torch.utils.data.Dataset for RTC data of the format (text1, text2, label),
    where the labels are as follows:
        0   Neutral
        1   Entailment
        2   Contradiction

    `__getitem__()` returns a dict containing ???
        TODO: Figure out how to pass both texts into the model (separate by
        [CLS] text1 [SEP] text2 [SEP], add segment_ids etc.)
    """

    def __init__(
        self,
        from_dataframe_pkl_path: str = RTC_DATAFRAME_PATH,
        tokenizer: str = "bert-base-uncased",
    ):
        logging.info("Initializing RelationshipTypeClassificationDataset...")
        df = None
        if os.path.exists(from_dataframe_pkl_path):
            df = pd.read_pickle(from_dataframe_pkl_path)
        else:
            raise Exception("RTC dataframe does not exist\nRun argument_graphs/data_preprocessing.py")
        self.data_frame = df.dropna()

        self.tokenizer = None
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)
        except:
            raise Exception("Invalid model name passed to transformers.AutoTokenizer")

        # Run tokenization for all text pairs during dataset initialization for
        # faster training and to ensure same tensor sizes (via padding and
        # truncation computed over the entire dataset)
        logging.info("Running dataset tokenization for RelationshipTypeClassificationDataset...")
        self.encoded_texts = [
            # We pass the pair of sentences to the tokenizer as the first and
            # second arguments; it will then automatically produce
            # `[CLS] text1 [SEP] text2 [PAD] ...` and properly set the
            # `token_type_ids` (0 for text1, 1 for text2, then 0 again for the
            # remaining padding)
            # https://discuss.huggingface.co/t/use-two-sentences-as-inputs-for-sentence-classification/5444/4
            self.tokenizer(
                text1, text2,
                padding="max_length", max_length=512, truncation=True,
                return_tensors="pt"
            ) for text1, text2 in zip(self.data_frame['text1'], self.data_frame['text2'])
        ]

    def __len__(self):
        """ Returns the size of the dataset; called as len(dataset) """
        return len(self.data_frame.index)

    def __getitem__(self, idx: int):

        if torch.is_tensor(idx):
            idx = idx.tolist()

        encoded_inputs = self.encoded_texts[idx]
        # TODO: Do we need to .squeeze() tensor dimensions here too?

        label = torch.Tensor(
            [self.data_frame.loc[idx, 'label']]
        ).long().squeeze()

        item = {
            "text": encoded_inputs,
            "label": label,
        }
        return item

if __name__ == "__main__":
    print("WARNING: This is only for testing argument_graphs/dataloader.py")
    print("\t This file should otherwise not be run directly")
    print("\t Run from root as")
    print("\t```")
    print("\t\tpython -m argument_graphs.dataloader")
    print("\t```")

    dataset = ArgumentativeUnitClassificationDataset()
    print(type(dataset))
    print(f"Dataset size: {len(dataset)}\n")
    for idx, item in enumerate(dataset):
        text, label = item['text'], item['label']
        print(text); print(label); print("")
        if idx > 10: break

    dataset = RelationshipTypeClassificationDataset()
    print(type(dataset))
    print(f"Dataset size: {len(dataset)}\n")
    for idx, item in enumerate(dataset):
        text, label = item['text'], item['label']
        print(text); print(label); print("")
        if idx > 10: break