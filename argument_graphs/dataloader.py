import sys
import os
import logging

import pandas as pd
from tqdm import tqdm
from pprint import pprint

import torch
from torch.utils.data import Dataset, DataLoader

from transformers import AutoTokenizer

logging.basicConfig(level=logging.DEBUG) # DEBUG, INFO, WARNING, ERROR, CRITICAL

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATA_PATH = "./data/ArgumentativeUnitClassification"
AUC_DATA_FILE = "all_auc_data.tsv"
AUC_DATA_PATH = os.path.join(DATA_PATH, AUC_DATA_FILE)
AUC_DATAFRAME_FILE = "auc_dataframe.pkl"
AUC_DATAFRAME_PATH = os.path.join(DATA_PATH, AUC_DATAFRAME_FILE)
class ArgumentativeUnitClassificationDataset(Dataset):
    """
    torch.utils.data.Dataset for AUC data of the format (text, label), where
    the labels are as follows:
        0   Non-argumentative unit
        1   Claim
        2   Premise

    `__getitem__()` returns a dict containing "text" and "label"
        text:   Encoded input tensor for sequence classification model; encoded
            by the specified tokenizer (which is a `transformers.AutoTokenizer`);
            default is `bert-base-uncased`
        label:  Label tensor (0, 1, or 2)
    """

    def __init__(
        self,
        from_dataframe_pkl_path: str = AUC_DATAFRAME_PATH,
        tokenizer: str = "bert-base-uncased",
    ):
        df = None
        if os.path.exists(from_dataframe_pkl_path):
            df = pd.read_pickle(from_dataframe_pkl_path)
        else:
            raise Exception("AUC dataframe does not exist\nRun argument_graphs/data_preprocessing.py")
        self.data_frame = df

        self.tokenizer = None
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)
        except:
            raise Exception("Invalid model name passed to transformers.AutoTokenizer")

        # Run tokenization for all text during dataset initialization for
        # faster training and to ensure same tensor sizes (via padding and
        # truncation computed over the entire dataset)
        self.encoded_texts = [
            self.tokenizer(
                text, padding="max_length", max_length=512, truncation=True, return_tensors="pt"
            ) for text in self.data_frame['text']
        ]

    def __len__(self):
        """ Returns the size of the dataset; called as len(dataset) """
        return len(self.data_frame.index)

    def __getitem__(self, idx: int):
        """ Returns a dict containing "text" (encoded tensor) and "label" """
        if torch.is_tensor(idx):
            idx = idx.tolist()

        text = self.data_frame.loc[idx, 'text']
        # encoded_inputs = self.tokenizer(
        #     text, padding=True, truncation=True, return_tensors="pt"
        # )
        encoded_inputs = self.encoded_texts[idx]
        # Note: When passing the direct output of `transformers.AutoTokenizer()`
        # to `transformers.AutoModelForSequenceClassification`, the error
        # `ValueError: too many values to unpack (expected 2)` comes up;
        # This is an issue with there being an additional dimension in the
        # three tensors generated by the tokenizer; the model expects the
        # dimension to be (batch_size, seq_len), but the tokenizer returns
        # (1, batch_size, seq_len); thus, we `.squeeze()` to get rid of that
        # extra dimension
        # https://stackoverflow.com/questions/67193312/huggingface-transformers-returning-valueerror-too-many-values-to-unpack-expec
        encoded_inputs = {
            'input_ids': encoded_inputs['input_ids'].squeeze(),
            'token_type_ids': encoded_inputs['token_type_ids'].squeeze(),
            'attention_mask': encoded_inputs['attention_mask'].squeeze(),
        }
        label = torch.Tensor(
            [self.data_frame.loc[idx, 'label']]
        ).long().squeeze()
        # label = self.data_frame.loc[idx, 'label']

        item = {
            "text": encoded_inputs,
            "label": label,
        }
        return item

if __name__ == "__main__":
    print("WARNING: This is only for testing argument_graphs/dataloader.py")
    print("\t This file should otherwise not be run directly")

    dataset = ArgumentativeUnitClassificationDataset()
    print(type(dataset))
    print(f"Dataset size: {len(dataset)}\n")
    for idx, item in enumerate(dataset):
        text, label = item['text'], item['label']
        print(text); print(label); print("")
        if idx > 10: break
