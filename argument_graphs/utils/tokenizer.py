"""
Utilities for tokenizing text input, specifically to properly process the
tensors generated by a given `transformers.AutoTokenizer` (to then be passed to
a `transformers.AutoModel`)
"""

from typing import Union, List

from transformers import AutoTokenizer

def encode_single_inputs(
    text: Union[str, List[str]],
    tokenizer: AutoTokenizer
):
    encoded_inputs = tokenizer(
        text,
        padding="max_length",
        max_length=512,
        truncation=True,
        return_tensors="pt"
    )

    # The encoded inputs should have 2 dimensions (e.g. [3, 512])
    # If they do not, we do the step below
    if len(encoded_inputs['input_ids'].size()) == 2:
        return encoded_inputs

    # Note: When passing the direct output of `transformers.AutoTokenizer()`
    # to `transformers.AutoModelForSequenceClassification`, the error
    # `ValueError: too many values to unpack (expected 2)` comes up;
    # This is an issue with there being an additional dimension in the
    # three tensors generated by the tokenizer; the model expects the
    # dimension to be (batch_size, seq_len), but the tokenizer returns
    # (1, batch_size, seq_len); thus, we `.squeeze()` to get rid of that
    # extra dimension
    # https://stackoverflow.com/questions/67193312/huggingface-transformers-returning-valueerror-too-many-values-to-unpack-expec
    if 'input_ids' in encoded_inputs:
        encoded_inputs['input_ids'] = encoded_inputs['input_ids'].squeeze()
    # RoBERTa does not have token_type_ids, so we check if that key exists
    if 'token_type_ids' in encoded_inputs:
        encoded_inputs['token_type_ids'] = encoded_inputs['token_type_ids'].squeeze()
    if 'attention_mask' in encoded_inputs:
        encoded_inputs['attention_mask'] = encoded_inputs['attention_mask'].squeeze()

    return encoded_inputs

def encode_paired_inputs(
    text1: Union[str, List[str]],
    text2:Union[str, List[str]],
    tokenizer: AutoTokenizer
):
    encoded_inputs = tokenizer(
        text1,
        text2,
        padding="max_length",
        max_length=512,
        truncation=True,
        return_tensors="pt"
    )

    # The encoded inputs should have 2 dimensions (e.g. [3, 512])
    # If they do not, we do the step below
    if len(encoded_inputs['input_ids'].size()) == 2:
        return encoded_inputs

    # Note: When passing the direct output of `transformers.AutoTokenizer()`
    # to `transformers.AutoModelForSequenceClassification`, the error
    # `ValueError: too many values to unpack (expected 2)` comes up;
    # This is an issue with there being an additional dimension in the
    # three tensors generated by the tokenizer; the model expects the
    # dimension to be (batch_size, seq_len), but the tokenizer returns
    # (1, batch_size, seq_len); thus, we `.squeeze()` to get rid of that
    # extra dimension
    # https://stackoverflow.com/questions/67193312/huggingface-transformers-returning-valueerror-too-many-values-to-unpack-expec
    if 'input_ids' in encoded_inputs:
        encoded_inputs['input_ids'] = encoded_inputs['input_ids'].squeeze()
    # RoBERTa does not have token_type_ids, so we check if that key exists
    if 'token_type_ids' in encoded_inputs:
        encoded_inputs['token_type_ids'] = encoded_inputs['token_type_ids'].squeeze()
    if 'attention_mask' in encoded_inputs:
        encoded_inputs['attention_mask'] = encoded_inputs['attention_mask'].squeeze()

    return encoded_inputs
