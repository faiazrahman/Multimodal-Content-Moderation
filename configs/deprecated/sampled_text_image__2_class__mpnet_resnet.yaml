model: "text_image_resnet_model" # The exact filename that the model is stored in, excluding the .py extension
modality: "text-image" # "text" | "image" | "text-image" | "text-image-dialogue"
num_classes: 2 # 2 | 3 | 6
batch_size: 64 # 64 # (int) Increase this as much as you can to maximize GPU utilization without exceeding CUDA memory
learning_rate: 1.0e-4 # (float) Note that the mantissa must have a decimal point to be parsed by YAML as a float (and not a str)
num_epochs: 2 # (int)
dropout_p: 0.1 # (float)
fusion_output_size: 512 # (int) Dimension after multi-modal embeddings fusion
gpus: [0, 1] # [0] | [1] | [0, 1] Note that it must be a list of ints
num_cpus: 0 # 0 for no multi-processing (during dataset batching), 24 for Yale Tangra server, 40 for Yale Ziva server
text_embedder: "all-mpnet-base-v2" # "all-mpnet-base-v2" | "all-distilroberta-v1"
image_encoder: "resnet" # "resnet" | "dino"
train_data_path: "./data/Fakeddit/multimodal_train_10000_sampled.tsv" # (str)
test_data_path: "./data/Fakeddit/multimodal_test_public_1000_sampled.tsv" # (str)
preprocessed_train_dataframe_path: "./data/Fakeddit/sampled_train__text_image__dataframe.pkl" # (str)
preprocessed_test_dataframe_path: "./data/Fakeddit/sampled_test__text_image__dataframe.pkl" # (str)
trained_model_version: 35 # (int)
trained_model_path: null # (str) Not needed if you specify trained_model_version and it's in lightning_logs/
