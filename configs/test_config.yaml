# Used in test_train_script_args_config.py (just to test that configs and command-line arg overriding work properly)
modality: "text-image-dialogue" # "text" | "image" | "text-image" | "text-image-dialogue"
num_classes: 3 # 2 | 3 | 6
batch_size: 4 # (int)
learning_rate: 5 # (float) Note that the mantissa must have a decimal point to be parsed by YAML as a float (and not a str)
num_epochs: 6 # (int)
gpus: [7, 8] # [0] | [1] | [0, 1] Note that it must be a list of ints
text_embedder: "9" # "all-mpnet-base-v2" | "all-distilroberta-v1"
dialogue_summarization_model: "10" # None=Transformers.Pipeline default i.e. "sshleifer/distilbart-cnn-12-6" | "bart-large-cnn" | "t5-small" | "t5-base" | "t5-large"
train_data_path: "11" # (str)
test_data_path: "./data/Fakeddit/multimodal_test_1000.tsv" # (str)
preprocessed_train_dataframe_path: "12" # (str)
preprocessed_test_dataframe_path: "./data/Fakeddit/test__text_image__dataframe.pkl" # (str)
trained_model_version: 1 # (int)
trained_model_path: null # (str) Not needed if you specify trained_model_version and it's in lightning_logs/
